{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7614a948",
   "metadata": {},
   "source": [
    "# Wrangle Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd8183b",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"#intro\">1. Introduction</a></li>\n",
    "<li><a href=\"#data-gathering\">2. Data Gathering</a></li>\n",
    "<li><a href=\"#assessing-data\">3. Assessing Data</a></li>\n",
    "<li><a href=\"#cleaning-data\">4. Cleaning Data</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1d2ab0",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This report describes the data wrangling process that was used to gather, assess and clean WeRateDogs data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34787f",
   "metadata": {},
   "source": [
    "<a id='data-gathering'></a>\n",
    "\n",
    "## 2. Data Gathering\n",
    "\n",
    "\n",
    "#### The WeRateDogs Twitter archive\n",
    "We have downloaded this file manually: `twitter_archive_enhanced.csv`. Once it was downloaded, we uploaded it and read the data into a pandas DataFrame.\n",
    "\n",
    "#### The tweet image predictions\n",
    "This file (image_predictions.tsv) is present in each tweet according to a neural network. It is hosted on Udacity's servers and was downloaded programmatically using the Requests library and the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "\n",
    "#### Additional data from the Twitter API\n",
    "We gathered each tweet's retweet count and favorite (\"like\") count. Using the tweet IDs in the WeRateDogs Twitter archive, we queried the Twitter API for each tweet's JSON data using Python's Tweepy library and stored each tweet's entire set of JSON data in a file called tweet_json.txt file.\n",
    "\n",
    "Each tweet's JSON data should be written to its own line. Then we read this .txt file line by line into a pandas DataFrame with tweet ID, retweet count, and favorite count.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247c6768",
   "metadata": {},
   "source": [
    "<a id='assessing-data'></a>\n",
    "\n",
    "## 3. Assessing Data\n",
    "\n",
    "Using both visual assessment and programmatic assessments, we were able to identify the following issues:\n",
    "\n",
    "### Quality Issues\n",
    "- Unwanted records that represent retweets replies.\n",
    "- Non common dog names in the name column, such as 'a', 'o', 'actually', 'all', 'an', 'by', 'getting', 'his', 'incredibly', 'infuriating', 'just', 'life', 'light', 'mad', 'my', 'not', 'officially', 'old', 'one', 'quite', 'space', 'such', 'the', 'this', 'unacceptable', 'very'.\n",
    "- Wrong datatype of tweet_id, which is int64.\n",
    "- Wrong datatype of timestamp, which is object.\n",
    "- Some records have missing values in the expanded_urls column.\n",
    "- The types of dogs in columns p1, p2, and p3 don't have the same format, some of them are lowercase, others are titlecase.\n",
    "- Wrong datatype of tweet_id, which is int64.\n",
    "- There are 2075 records in the image predictions dataset, meaning that we have 281 missing records, because there are 2356 records in the Data Archive dataset.\n",
    "- Wrong datatype of id_str, which is int64.\n",
    "- There are 2325 records present in the additional JSON dataset. meaning that we have 31 missing records.\n",
    "\n",
    "### Tidiness Issues\n",
    "- The columns doggo, floofer, pupper and puppo are unnecessary. The names of dogs should be stored in one column which is name.\n",
    "- Unnecessary columns : retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp, in_reply_to_status_id, in_reply_to_user_id, rating_denominator, and source.\n",
    "- Instead of keeping the three types that are in p1, p2, and p3, we only need the one with the heighest probability.\n",
    "- No need for records in which the image is predicted as a non dog (ie. p1_dog, p2_dog and p3_dog are all False).\n",
    "- No need for the img_num column.\n",
    "- The column name of id_str should be changed to tweet_id so that we can merege the three datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc8edc",
   "metadata": {},
   "source": [
    "<a id='cleaning-data'></a>\n",
    "\n",
    "## 4. Cleaning Data\n",
    "\n",
    "After making copies of the 3 datasets, we resolved the issues that we mentionned in the previous section using the following methods:\n",
    "- Dropped unwanted columns.\n",
    "- Dropped records that represent replies or retweets.\n",
    "- Dropped records that have unusual dog names.\n",
    "- Converted columns datatypes to what they should be using <b>df.apply</b> and <b>pd.to_datetime</b> methods.\n",
    "- Dropped records with missing values in the `expanded_urls` column.\n",
    "- Converted all dog breeds to lowercase.\n",
    "- Stored dogs names in the `name` column and dropped the following columns `doggo`, `floofer`, `pupper` and `puppo`.\n",
    "- Selected only the most confident dog breed level.\n",
    "- Dropped records which are identified as non dogs by the neural network predictions.\n",
    "\n",
    "Finally we combined the three datasets into one dataset `df_master` and saved it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
